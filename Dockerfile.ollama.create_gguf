FROM ollama/ollama:latest

# ðŸ”¹ Modell neve vÃ¡ltozÃ³kÃ©nt
ARG MODEL_NAME=mistral-ai/mistral-small-24b
ARG MODEL_FILE_NAME=mistral-ai/mistral-small-24b
ARG MODEL_ID=mistral-ai/mistral-small-24b

# ðŸ”¹ AlapvetÅ‘ csomagok telepÃ­tÃ©se + letÃ¶ltÃ©s + konverziÃ³ + takarÃ­tÃ¡s egyetlen RUN-ban
#        && python /root/llama.cpp/convert-hf-to-gguf.py /root/ollama_models --outfile /root/ollama_models/${MODEL_FILE_NAME} --outtype q8_0 \
RUN apt update && apt install -y \
    bash procps coreutils git-lfs curl python3 python3-pip git cmake make g++ jq \
    && ln -s /usr/bin/python3 /usr/bin/python \
    && pip install --no-cache-dir huggingface_hub torch numpy transformers sentencepiece \
    && mkdir -p /root/.ollama/models/blobs /root/ollama_models \
    && if [ ! -f "/root/.ollama/models/blobs/${MODEL_FILE_NAME}" ]; then \
        echo "âš ï¸ GGUF fÃ¡jl nem talÃ¡lhatÃ³, letÃ¶ltÃ©s Ã©s konverziÃ³..."; \
        huggingface-cli download ${MODEL_NAME} --local-dir /root/ollama_models \
        && git clone https://github.com/ggerganov/llama.cpp.git /root/llama.cpp \
        && cd /root/llama.cpp && git pull \
        && mkdir build && cd build && cmake .. && make -j$(nproc) \
        && echo "ðŸ“‚ MODEL CONFIG:" && cat /root/ollama_models/config.json || echo "âš ï¸ config.json hiÃ¡nyzik!" \
        && if ! jq -e '.architectures' /root/ollama_models/config.json > /dev/null; then \
            echo "âš ï¸ architectures hiÃ¡nyzik! HozzÃ¡adÃ¡s..."; \
            jq '. + { "architectures": ["LlamaForCausalLM"] }' /root/ollama_models/config.json > /root/ollama_models/config_fixed.json \
            && mv /root/ollama_models/config_fixed.json /root/ollama_models/config.json; \
        fi \
        && python /root/llama.cpp/convert_hf_to_gguf.py /root/ollama_models --outfile /root/ollama_models/${MODEL_FILE_NAME} --outtype q8_0 \
        && mv /root/ollama_models/*.gguf /root/.ollama/models/blobs/ \
        && rm -rf /root/ollama_models /root/llama.cpp; \
    else \
        echo "âœ”ï¸ GGUF modell mÃ¡r lÃ©tezik, konverziÃ³ kihagyva."; \
    fi \
    && rm -rf /var/lib/apt/lists/*

# ðŸ”¹ Modell regisztrÃ¡lÃ¡sa Ollama-ban
RUN ollama serve & sleep 15 \
    && echo "FROM /root/.ollama/models/blobs/${MODEL_FILE_NAME}" > /root/Modelfile \
    && ollama create ${MODEL_ID} -f /root/Modelfile \
    && pkill ollama

# ðŸ”¹ TovÃ¡bbi modellek letÃ¶ltÃ©se
RUN ollama serve & sleep 15 \
    && ollama pull llama3.2-vision \
    && ollama pull bakllava \
    && ollama pull llava-llama3 \
    && ollama pull deepseek-r1 \
    && ollama pull mistral-small:24b \
    && pkill ollama

# ðŸ”¹ KÃ¶rnyezeti vÃ¡ltozÃ³k
ENV OLLAMA_LOAD_TIMEOUT=30m  
ENV OLLAMA_KEEP_ALIVE=10m    
ENV OLLAMA_MAX_QUEUE=1024    
ENV GIN_MODE=release

# ðŸ”¹ AlapÃ©rtelmezett ENTRYPOINT
ENTRYPOINT ["ollama"]

# ðŸ”¹ A szerver indÃ­tÃ¡sa
CMD ["serve"]


import os  # K√∂rnyezeti v√°ltoz√≥k kezel√©s√©hez
import json  # JSON v√°laszok feldolgoz√°s√°hoz
from typing import List, Dict, AsyncGenerator, Any  # T√≠pusannot√°ci√≥khoz
import httpx  # HTTP k√©r√©sekhez
from fastapi import HTTPException  # Hibakezel√©shez 
import logging  # Napl√≥z√°shoz

logger = logging.getLogger(__name__)

class ModelManager:
    """
    ModelManager oszt√°ly arra, hogy a v√°lasztott modell (OpenAI vagy Ollama) alapj√°n
    gener√°ljon streamelt v√°laszt, teljes v√°laszt, valamint kezelje a Tool Calling funkci√≥kat.
    """
    def __init__(self, model_type: str, model_name: str, api_url: str = None):
        self.model_type = model_type
        self.model_name = model_name
        self.embedding_size = 768  # Alap√©rtelmezett √©rt√©k
        
        if self.model_type == "ollama":
            self.embedding_size = 768
        elif self.model_type == "openai":
            self.embedding_size = 1536
        elif self.model_type == "groq":
            self.embedding_size = 1536  # Groq is using OpenAI-compatible models
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")

        # Groq url/key
        self.groq_api_url = os.getenv("GROQ_API_URL", "https://api.groq.com/openai/v1/chat/completions")
        self.groq_api_key = os.getenv("GROQ_API_KEY", "")

        # Ollama url/key
        self.ollama_api_url = os.getenv("OLLAMA_CHAT_API_URL", "http://localhost:11434/api/generate")
        
        # OpenAI url/key        
        self.openai_api_url = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
        self.openai_api_key = os.getenv("OPENAI_API_KEY", "")

        self.deepseek_api_url = os.getenv("DEEPSEEK_API_URL", "https://api.deepseek.com/v1/chat/completions")
        self.deepseek_api_key = os.getenv("DEEPSEEK_API_KEY", "")


    async def embed_query(self, text: str) -> List[float]:
        """
        Embed sz√∂veg gener√°l√°sa a kiv√°lasztott modell alapj√°n.
        """
        if self.model_type == "ollama":
            url = os.getenv("OLLAMA_EMBEDDING_API_URL", "http://localhost:11434/api/embeddings")
            payload = {"model": "nomic-embed-text", "prompt": text}
            async with httpx.AsyncClient(timeout=600) as client:
                response = await client.post(url, json=payload)
                if response.status_code == 200:
                    result = response.json()
                    return result["embedding"]
                else:
                    raise Exception(f"Ollama embedding hiba: {response.status_code}, {response.text}")
        elif self.model_type == "openai":
            # OpenAI embedding logika
            headers = {"Authorization": f"Bearer {self.openai_api_key}"}
            payload = {
                "model": "text-embedding-ada-002",  # Az OpenAI aj√°nlott embedding modellje
                "input": text
            }

            async with httpx.AsyncClient(timeout=600) as client:
                response = await client.post(self.openai_api_url.replace("/chat/completions", "/embeddings"),
                                             headers=headers, json=payload)
                if response.status_code == 200:
                    result = response.json()
                    return result['data'][0]['embedding']
                else:
                    raise Exception(f"OpenAI embedding hiba: {response.status_code}, {response.text}")
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")


    async def generate_stream(self, model_type, model_name, messages: List[Dict[str, str]]) -> AsyncGenerator[dict, None]:
        if model_type == "ollama":
            async for chunk in self._generate_ollama_stream(model_name,messages):
                yield chunk
        elif model_type == "openai":
            async for chunk in self._generate_openai_stream(model_name,messages):
                yield chunk
        elif model_type == "groq":
            async for chunk in self._generate_groq_stream(model_name,messages):
                yield chunk
        elif model_type == "deepseek":
            async for chunk in self._generate_deepseek_stream(model_name,messages):
                yield chunk
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")

    async def _generate_ollama_stream(self, model_name, messages: List[Dict[str, str]]) -> AsyncGenerator[dict, None]:
        """
        Ollama modell streaming.
        A v√°laszt OpenAI-kompatibilis form√°tumra alak√≠tjuk.
        """
        prompt = "\n".join([f"{m['role']}: {m['content']}" for m in messages])

        async with httpx.AsyncClient(timeout=600) as client:
            try:
                logger.info(f"Prompt: {prompt}")
                headers = {
                    "Content-Type": "application/json"
                }
                response = await client.post(
                    "http://ollama:11434/api/generate",
                    json={
                        "model": model_name,
                        "prompt": prompt,
                        "stream": True
                    },
                    timeout=600
                )
                logger.info(f"Ollama v√°lasz: {response.status_code}")

                async for line in response.aiter_lines():
                    logger.info(f"Kapott sor: {line}")
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        data = json.loads(line)
                    except json.JSONDecodeError:
                        logger.warning(f"Ollama v√°lasz nem JSON: {line}")
                        continue

                    logger.info(f"JSON adat: {data}")

                    # Stream v√©g√©nek kezel√©se
                    if data.get("done"):
                        yield {"choices": [{"delta": {}, "finish_reason": "stop"}]}
                        break

                    # Tartalom lek√©r√©se √©s √°talak√≠t√°sa OpenAI form√°tumra
                    content = data.get("response", "")
                    if content:
                        yield {
                            "choices": [
                                {
                                    "delta": {"content": content},
                                    "finish_reason": None
                                }
                            ]
                        }
            except httpx.RequestError as e:
                logger.error(f"Ollama request error: {e}", exc_info=True)
                raise HTTPException(status_code=500, detail="Ollama API hiba.")

    async def _generate_openai_stream(self, model_name, messages: List[Dict[str, str]]) -> AsyncGenerator[dict, None]:
        headers = {"Authorization": f"Bearer {self.openai_api_key}"}
        body = {
            "model": model_name,
            "messages": messages,
            "stream": True,
            "temperature": 0.1,
            "top_p": 1,
            "frequency_penalty": 0,
            "presence_penalty": 0,
        }

        async with httpx.AsyncClient(timeout=600) as client:
            async with client.stream("POST", self.openai_api_url, headers=headers, json=body) as response:
                async for line in response.aiter_lines():
                    line = line.strip()
                    if not line or line == "data: [DONE]":
                        continue  # √úres sorokat √©s befejez≈ë jelz√©st kihagyjuk

                    # Ha a sor "data: " el≈ëtaggal kezd≈ëdik, t√°vol√≠tsuk el
                    if line.startswith("data: "):
                        line = line[6:].strip()

                    try:
                        data = json.loads(line)
                        #logger.info(data)
                        if "choices" in data and data["choices"][0]["finish_reason"] is not None:
                            break  # Befejez√©si jelz√©s eset√©n kil√©p√ºnk
                        yield data
                    except json.JSONDecodeError:
                        logger.warning(f"Nem siker√ºlt JSON dek√≥dolni: {line}")


    async def _generate_groq_stream(self, model_name,messages: List[Dict[str, str]]) -> AsyncGenerator[dict, None]:
        headers = {
            "Authorization": f"Bearer {self.groq_api_key}",
            "Content-Type": "application/json"
        }
        body = {
            "model": model_name,
            "messages": messages,
            "stream": True,
            "temperature": 0.1,
#            "max_tokens": 1000
        }
        async with httpx.AsyncClient(timeout=600) as client:
            async with client.stream("POST", self.groq_api_url, headers=headers, json=body) as response:
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = json.loads(line[6:])
                        #logger.info(data)
                        if data["choices"][0]["finish_reason"] is not None:
                            break
                        yield data

    async def _generate_deepseek_stream(self, model_name,messages: List[Dict[str, str]]) -> AsyncGenerator[dict, None]:
        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json"
        }
        body = {
            "model": model_name,
            "messages": messages,
            "stream": True
        }
        async with httpx.AsyncClient(timeout=600) as client:
            async with client.stream("POST", self.deepseek_api_url, headers=headers, json=body) as response:
                if response.status_code != 200:
                    error_text = await response.aread()  # üî• Olvasd ki a teljes v√°laszt
                    logger.error(f"DeepSeek API hib√°s k√©r√©s: {error_text.decode()}", exc_info=True)  # üî• Napl√≥zd az √ºzenetet
                    raise HTTPException(status_code=400, detail=f"DeepSeek API Bad Request: {error_text.decode()}")
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = json.loads(line[6:])
                        #logger.info(data)
                        if "choices" in data and data["choices"][0]["finish_reason"] is not None:
                            break
                        yield data

    async def generate_complete(self, model_type, model_name, messages: List[Dict[str, str]]) -> str:
        """
        A kiv√°lasztott modellnek elk√ºldi a k√©r√©st √©s a teljes v√°laszt adja vissza egyben.
        """
        if model_type == "ollama":
            return await self._generate_ollama_complete(model_name, messages)
        elif model_type == "openai":
            return await self._generate_openai_complete(model_name, messages)
        elif model_type == "groq":
            return await self._generate_groq_complete(model_name, messages)
        elif model_type == "deepseek":
            return await self._generate_deepseek_complete(model_name, messages)
        else:
            raise ValueError(f"Unsupported model type: {model_type}")

    async def _generate_ollama_complete(self, model_name,messages: List[Dict[str, str]]) -> str:
        prompt = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
        async with httpx.AsyncClient(timeout=600) as client:
            response = await client.post(
                self.ollama_api_url,
                json={
                    "model": model_name,
                    "prompt": prompt,
#                    "options": {"num_ctx": 60000}
                },
                timeout=600
            )
            
            result = ""
            async for line in response.aiter_lines():
                line = line.strip()
                if not line:
                    continue

                try:
                    data = json.loads(line)
                    result += data.get("response", "")
                except json.JSONDecodeError:
                    logger.warning(f"Skipping non-JSON line: {line}")
            
            return result

    async def _generate_openai_complete(self, model_name, messages: List[Dict[str, str]]) -> str:
        """
        OpenAI modell teljes v√°lasz gener√°l√°sa.
        """
        headers = {"Authorization": f"Bearer {self.openai_api_key}"}
        body = {
            "model": model_name,
            "messages": messages,
            "temperature": 0.1,
            "top_p": 1,
            "frequency_penalty": 0,
            "presence_penalty": 0,
        }
        async with httpx.AsyncClient(timeout=600) as client:
            response = await client.post(self.openai_api_url, headers=headers, json=body, timeout=None)
            response_data = response.json()
            if response.status_code != 200 or "choices" not in response_data:
                raise ValueError(f"OpenAI API hiba: {response.text}")
            return response_data["choices"][0]["message"]["content"]

    async def _generate_groq_complete(self, model_name, messages: List[Dict[str, str]]) -> str:
        """
        Groq modell teljes v√°lasz gener√°l√°sa.
        """
        headers = {
            "Authorization": f"Bearer {self.groq_api_key}",
            "Content-Type": "application/json"
        }
        body = {
            "model": model_name,
            "messages": messages,
            "temperature": 0.1,
#            "max_tokens": 60000
        }
        logger.info(headers, exc_info=True)
        logger.info(body, exc_info=True)
        logger.info(self.groq_api_url, exc_info=True)
        async with httpx.AsyncClient(timeout=600) as client:
            response = await client.post(self.groq_api_url, headers=headers, json=body, timeout=None)
            logger.info(response, exc_info=True)
            response_data = response.json()
            logger.info(response_data, exc_info=True)
            if response.status_code != 200 or "choices" not in response_data:
                raise ValueError(f"Groq API hiba: {response.text}")
            logger.info(response_data["choices"][0]["message"]["content"])
            return response_data["choices"][0]["message"]["content"]

    async def _generate_deepseek_complete(self, messages: List[Dict[str, str]]) -> str:
        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json"
        }
        body = {
            "model": model_name,
            "messages": messages,
            "stream": False
        }
        async with httpx.AsyncClient(timeout=600) as client:
            response = await client.post(self.deepseek_api_url, headers=headers, json=body, timeout=None)
            response_data = response.json()
            if response.status_code != 200 or "choices" not in response_data:
                raise ValueError(f"DeepSeek API hiba: {response.text}")
            return response_data["choices"][0]["message"]["content"]

    # Met√≥dus a Tool Calling funkci√≥hoz
    """
    Haszn√°lat:

    Eszk√∂z√∂k vagy funkci√≥k defini√°l√°sa:

    OpenAI eset√©ben:

    functions = [
        {
            "name": "get_current_weather",
            "description": "Lek√©ri az aktu√°lis id≈ëj√°r√°st egy adott v√°rosban.",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "A v√°ros neve, amelynek id≈ëj√°r√°s√°t le kell k√©rni.",
                    },
                },
                "required": ["city"],
            },
        },
    ]
    Ollama eset√©ben:

    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Lek√©ri az aktu√°lis id≈ëj√°r√°st egy adott v√°rosban.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "A v√°ros neve, amelynek id≈ëj√°r√°s√°t le kell k√©rni.",
                        },
                    },
                    "required": ["city"],
                },
            },
        },
    ]
    K√©r√©s k√ºld√©se a Tool Calling met√≥dushoz:

    OpenAI eset√©ben:

    model_manager = ModelManager(model_type="openai", model_name="gpt-3.5-turbo")
    result = await model_manager.generate_tool_calling(messages, functions)
    Ollama eset√©ben:

    model_manager = ModelManager(model_type="ollama", model_name="your-ollama-model")
    result = await model_manager.generate_tool_calling(messages, tools)
    """
    async def generate_tool_calling(self, messages: List[Dict[str, str]], functions_or_tools: List[Dict[str, Any]]) -> str:
        """
        A kiv√°lasztott modellnek elk√ºldi a k√©r√©st a Tool Calling funkci√≥val √©s a teljes v√°laszt adja vissza egyben.
        """
        if self.model_type == "ollama":
            return await self._generate_ollama_tool_calling(messages, functions_or_tools)
        elif self.model_type == "openai":
            return await self._generate_openai_tool_calling(messages, functions_or_tools)
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")

    async def _generate_openai_tool_calling(self, messages: List[Dict[str, str]], functions: List[Dict[str, Any]]) -> str:
        """
        OpenAI modell Tool Calling (Function Calling) t√°mogat√°sa.
        """
        headers = {"Authorization": f"Bearer {self.openai_api_key}"}
        body = {
            "model": self.model_name,
            "messages": messages,
            "functions": functions,
            "temperature": 0.1,
            "top_p": 1,
            "frequency_penalty": 0,
            "presence_penalty": 0,
        }
        async with httpx.AsyncClient(timeout=600) as client:
            response = await client.post(self.openai_api_url, headers=headers, json=body, timeout=None)
            response_data = response.json()
            if response.status_code != 200 or "choices" not in response_data:
                raise ValueError(f"OpenAI API hiba: {response.text}")

            message = response_data["choices"][0]["message"]
            # Ellen≈ërizz√ºk, hogy a modell h√≠v-e funkci√≥t
            if "function_call" in message:
                function_call = message["function_call"]
                function_name = function_call["name"]
                arguments = json.loads(function_call["arguments"])
                result = await self.call_tool(function_name, arguments)
                # A funkci√≥ eredm√©ny√©t visszaadjuk a felhaszn√°l√≥nak
                return result
            else:
                # Ha nincs funkci√≥h√≠v√°s, akkor a modell v√°lasz√°t adjuk vissza
                return message.get("content", "")

    async def _generate_ollama_tool_calling(self, messages: List[Dict[str, str]], tools: List[Dict[str, Any]]) -> str:
        """
        Ollama modell Tool Calling t√°mogat√°sa.
        """
        prompt = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
        async with httpx.AsyncClient(timeout=600) as client:
            response = await client.post(
                self.ollama_api_url,
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "tools": tools,
                    "options": {"num_ctx": 60000}
                },
                timeout=600
            )
            
            result = ""
            async for line in response.aiter_lines():
                line = line.strip()
                if not line:
                    continue

                try:
                    data = json.loads(line)
                except json.JSONDecodeError:
                    logger.warning(f"Skipping non-JSON line: {line}")
                    continue

                # Ellen≈ërizz√ºk, hogy van-e eszk√∂zh√≠v√°s
                if 'tool_calls' in data:
                    for tool_call in data['tool_calls']:
                        tool_name = tool_call['function']['name']
                        arguments = tool_call['function']['arguments']
                        # Megh√≠vjuk a megfelel≈ë eszk√∂zt
                        result_from_tool = await self.call_tool(tool_name, arguments)
                        # Az eszk√∂z eredm√©ny√©t hozz√°adjuk a v√°laszhoz
                        result += result_from_tool
                else:
                    # Ha nincs eszk√∂zh√≠v√°s, a modell v√°lasz√°t adjuk hozz√°
                    result += data.get("response", "")
            
            return result


    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> str:
        """
        Megh√≠vja a megfelel≈ë eszk√∂zt a megadott n√©vvel √©s argumentumokkal.
        """
        # Implement√°lja az eszk√∂z√∂k funkci√≥it itt
        if tool_name == "get_current_weather":
            city = arguments.get("city", "")
            return await self.get_current_weather(city)
        # Tov√°bbi eszk√∂z√∂k hozz√°ad√°sa sz√ºks√©g eset√©n
        else:
            logger.warning(f"Ismeretlen eszk√∂z: {tool_name}")
            return ""

    async def get_current_weather(self, city: str) -> str:
        """
        P√©lda eszk√∂z f√ºggv√©ny, amely lek√©ri az aktu√°lis id≈ëj√°r√°st egy adott v√°rosban.
        """
        # Itt implement√°lhatja a t√©nyleges API h√≠v√°st egy id≈ëj√°r√°s szolg√°ltat√°shoz
        # Demonstr√°ci√≥k√©nt egy p√©lda v√°laszt adunk vissza
        return f"A(z) {city} v√°rosban jelenleg napos az id≈ë, 25¬∞C h≈ëm√©rs√©klettel."


    async def process_image(self, model_type: str, model_name: str, messages: List[Dict[str, str]]) -> str:
        """
        K√©p OCR feldolgoz√°s OpenAI vagy Ollama seg√≠ts√©g√©vel.
        """
        if model_type == "ollama":
            return await self._ollama_image_processing(model_name, messages)
        elif model_type == "openai":
            return await self._openai_image_processing(model_name, messages)
        elif model_type == "groq":
            return await self._groq_image_processing(model_name, messages)
        raise ValueError(f"Unsupported model type for image: {self.model_type}")

    async def _ollama_image_processing(self, model_name, messages: List[Dict[str, str]]) -> str:
        """
        Ollama OCR vagy k√©pfeldolgoz√°si feladatok v√©grehajt√°sa.
        """
        async with httpx.AsyncClient() as client:
            response = await client.post(
                self.ollama_api_url,
                json={"model": model_name, "messages": messages},
                timeout=600
            )

            if response.status_code != 200:
                logging.error(f"OCR Error: {response.status_code} {response.text}")
                return None

            # Iter√°ljunk az egyes JSON-objektumokon
            text_content = ""
            for line in response.text.splitlines():
                try:
                    json_obj = json.loads(line)
                    content = json_obj.get("message", {}).get("content", "")
                    text_content += content
                except json.JSONDecodeError as e:
                    logging.error(f"JSON decoding error: {e} | Raw line: {line}")
                    continue

            # T√©rj vissza a teljes OCR sz√∂veggel
            return text_content.strip() if text_content else None


    async def _openai_image_processing(self, model_name, messages: List[Dict[str, str]]) -> str:
        """
        OpenAI OCR vagy k√©pfeldolgoz√°si feladatok v√©grehajt√°sa.
        """
        headers = {"Authorization": f"Bearer {self.openai_api_key}"}
        prompt = messages[0].get("content")
        base64_image = messages[0].get("images")
        # Ellen≈ërizz√ºk, hogy a base64_image tartalmazza-e a data:image prefixet
        if not base64_image or not base64_image[0].startswith("data:image"):
            base64_image = f"data:image/jpeg;base64,{base64_image}"
        
        body = {
            "model": model_name,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": base64_image}}
                    ]
                }
            ]
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(self.openai_api_url, headers=headers, json=body, timeout=600)
            response_json = response.json()
            logging.info(response)
            # Ellen≈ërizz√ºk, hogy a v√°laszban van-e sz√∂veges tartalom
            return response_json["choices"][0]["message"]["content"] if "choices" in response_json else "No response received."



    async def _groq_image_processing(self, model_name, messages: List[Dict[str, str]]) -> str:
        if not messages:
            raise ValueError("A messages lista √ºres")

        headers = {
            "Authorization": f"Bearer {self.groq_api_key}",
            "Content-Type": "application/json"
        }

        prompt = messages[0].get("content")
        base64_image = messages[0].get("images")[0]

        if not base64_image:
            raise ValueError("A base64_image nem tal√°lhat√≥")

        # Ellen≈ërzi, hogy a base64_image sztring-e
        if not isinstance(base64_image, str):
            raise ValueError("A base64_image nem sztring")

        # Ellen≈ërzi, hogy a base64_image tartalmazza a data:image prefixet
        if not base64_image.startswith("data:image"):
            base64_image = f"data:image/jpeg;base64,{base64_image}"

        #logging.info(f"{type(base64_image)}, {base64_image[0]}")
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": base64_image}}
                ]
            }
        ]

        body = {
            "model": model_name,
            "messages": messages,
            "temperature" :1,
            "max_tokens" : 2048,
            "top_p" : 1,
            "stream": False,
            "stop" : None
        }

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(self.groq_api_url, headers=headers, json=body, timeout=600)
                response.raise_for_status()
                response_json = response.json()
                logging.info(f"V√°lasz kapott: {response_json}")
                if "choices" in response_json:
                    return response_json["choices"][0]["message"]["content"]
                else:
                    logging.warning("Nem siker√ºlt az API h√≠v√°s")
                    return "Nem siker√ºlt az API h√≠v√°s"
        except httpx.HTTPError as e:
            logging.error(f"HTTP Hib√°s: {e.response.text}")
            return f"HTTP Hib√°s: {e}"
        except Exception as e:
            logging.error(f"√Åltal√°nos hiba: {e}")
            return f"√Åltal√°nos hiba: {e}"  
  
